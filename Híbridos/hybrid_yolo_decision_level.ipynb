{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"S-ZKN7ZUle3J"},"outputs":[],"source":["# ==============================================\n","# Modelo H√≠brido 1 - Decision-Level Fusion\n","# Ensemble por soma de probabilidades\n","# =============================================="]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"bCrX20IFX9ko"},"outputs":[],"source":["!pip install ultralytics opencv-python pillow"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2320,"status":"ok","timestamp":1770311948772,"user":{"displayName":"Marcos Vin√≠cius","userId":"03427921222894808901"},"user_tz":180},"id":"rxDwlrvHk3iF","outputId":"9bce2545-e7ab-4be7-cf81-dc2bb1982527"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ABOPtd-MXEen"},"outputs":[],"source":["# ---------- Bibliotecas ----------\n","\n","import os\n","import cv2\n","import torch\n","import time\n","import numpy as np\n","from torch import nn\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","from itertools import product\n","import timm\n","from tqdm import tqdm\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from ultralytics import YOLO"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1770311948824,"user":{"displayName":"Marcos Vin√≠cius","userId":"03427921222894808901"},"user_tz":180},"id":"aB0z-M29N1tE","outputId":"78ce1234-cfc7-4187-d8fa-5a2d6dceef83"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["path = os.getcwd()\n","print(path)\n","\n","#os.chdir(path)\n","#file_log = open(path + \"/mensagem_final_classificar_V2.txt\", \"a\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDZ87AmYm0TX"},"outputs":[],"source":["# === Configura√ß√µes Gerais ===\n","\n","NUM_CLASSES = 15\n","INPUT_SIZE = 224\n","BATCH_SIZE = 16\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","DATASET_FOLHA = \"/content/drive/MyDrive/TCC/Datasets/Imagens Folhas/Especies\"\n","DATASET_CASCA = \"/content/drive/MyDrive/TCC/Datasets/Imagens tronco/EspeciesCascas\"\n","\n","PESOS_FOLHA = \"/content/drive/MyDrive/TCC/Datasets/main_weights/Folha/YOLOv12_best_leaf.pt\"\n","PESOS_CASCA = \"/content/drive/MyDrive/TCC/Datasets/main_weights/Casca/YOLOv12_best_bark.pt\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QuAqWEyUl_4I"},"outputs":[],"source":["def stratified_split(dataset, test_split=0.1, valid_split=0.2, seed=42):\n","    labels = [label for *_, label in dataset.samples]\n","\n","    sss1 = StratifiedShuffleSplit(\n","        n_splits=1,\n","        test_size=test_split,\n","        random_state=seed\n","    )\n","    train_valid_idx, test_idx = next(\n","        sss1.split(np.zeros(len(labels)), labels)\n","    )\n","\n","    labels_train_valid = np.array(labels)[train_valid_idx]\n","\n","    sss2 = StratifiedShuffleSplit(\n","        n_splits=1,\n","        test_size=valid_split,\n","        random_state=seed\n","    )\n","    train_idx, valid_idx = next(\n","        sss2.split(np.zeros(len(labels_train_valid)), labels_train_valid)\n","    )\n","\n","    train_idx = np.array(train_valid_idx)[train_idx]\n","    valid_idx = np.array(train_valid_idx)[valid_idx]\n","\n","    train_ds = torch.utils.data.Subset(dataset, train_idx)\n","    valid_ds = torch.utils.data.Subset(dataset, valid_idx)\n","    test_ds  = torch.utils.data.Subset(dataset, test_idx)\n","\n","    return train_ds, valid_ds, test_ds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJcSOMhglqNN"},"outputs":[],"source":["# ================= Dataset Base =================\n","class ImageFolderDataset(Dataset):\n","    def __init__(self, root_dir):\n","        self.samples = []\n","        classes = sorted(os.listdir(root_dir))\n","        self.class_to_idx = {c: i for i, c in enumerate(classes)}\n","\n","        for cls in classes:\n","            cls_path = os.path.join(root_dir, cls)\n","            if not os.path.isdir(cls_path):\n","                continue\n","            for f in os.listdir(cls_path):\n","                if f.lower().endswith(('.png', '.jpg', '.jpeg')):\n","                    self.samples.append((os.path.join(cls_path, f),\n","                                         self.class_to_idx[cls]))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def preprocess(self, img_path):\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, (INPUT_SIZE, INPUT_SIZE))\n","        img = img.astype(np.float32) / 255.0\n","\n","        mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","        std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","        img = (img - mean) / std\n","\n","        img = np.transpose(img, (2, 0, 1))\n","        return torch.from_numpy(img)\n","\n","    def __getitem__(self, idx):\n","        img_path, label = self.samples[idx]\n","        # leitura via OpenCV\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        # redimensionar\n","        img = cv2.resize(img, (INPUT_SIZE, INPUT_SIZE))\n","        # converter para float32 e normalizar [0,1]\n","        img = img.astype(np.float32) / 255.0\n","        # talvez normaliza√ß√£o adicional conforme modelo (media/std)\n","        # usando valores padr√£o ImageNet\n","        mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","        std  = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","        img = (img - mean) / std\n","        # mudar de H√óW√óC para C√óH√óW\n","        img = np.transpose(img, (2,0,1))\n","        img_tensor = torch.from_numpy(img)\n","        label_tensor = torch.tensor(label, dtype=torch.long)\n","        return img_tensor, label_tensor\n","\n","# ================= Dataset Bimodal =================\n","class CartesianFusionDataset(Dataset):\n","    def __init__(self, ds_folha, ds_casca):\n","        self.samples = []\n","\n","        # Extrair samples considerando Subset\n","        def get_samples(ds):\n","            if isinstance(ds, torch.utils.data.Subset):\n","                # Pegar apenas os √≠ndices do subset\n","                base_samples = ds.dataset.samples\n","                return [base_samples[i] for i in ds.indices]\n","            else:\n","                return ds.samples\n","\n","        folha_samples = get_samples(ds_folha)\n","        casca_samples = get_samples(ds_casca)\n","\n","        # Agrupar por classe\n","        folhas_por_classe = {}\n","        cascas_por_classe = {}\n","\n","        for img, label in folha_samples:\n","            if label not in folhas_por_classe:\n","                folhas_por_classe[label] = []\n","            folhas_por_classe[label].append(img)\n","\n","        for img, label in casca_samples:\n","            if label not in cascas_por_classe:\n","                cascas_por_classe[label] = []\n","            cascas_por_classe[label].append(img)\n","\n","        # Produto cartesiano por classe\n","        for label in folhas_por_classe.keys():\n","            if label not in cascas_por_classe:\n","                continue\n","            for f_img in folhas_por_classe[label]:\n","                for c_img in cascas_por_classe[label]:\n","                    self.samples.append((f_img, c_img, label))\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        folha_path, casca_path, label = self.samples[idx]\n","\n","        # Preprocessar imagens\n","        img_f = self.preprocess(folha_path)\n","        img_c = self.preprocess(casca_path)\n","\n","        return img_f, img_c, torch.tensor(label, dtype=torch.long)\n","\n","    def preprocess(self, img_path):\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, (INPUT_SIZE, INPUT_SIZE))\n","        img = img.astype(np.float32) / 255.0\n","\n","        mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","        std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","        img = (img - mean) / std\n","\n","        img = np.transpose(img, (2, 0, 1))\n","        return torch.from_numpy(img)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5lh5g94blr7G"},"outputs":[],"source":["# ================= Modelo Base =================\n","def create_model(num_classes=NUM_CLASSES, freeze_stages=3):\n","    \"\"\"\n","    Cria modelo YOLOv12 (backbone + classifier).\n","    DEVE ser ID√äNTICO ao usado no treinamento!\n","    \"\"\"\n","    # 1. Carregar YOLO base\n","    yolo = YOLO('yolo12n.pt')\n","    full_model = yolo.model\n","\n","    # 2. Extrair backbone\n","    backbone_end = 10  # default\n","    for i, layer in enumerate(full_model.model):\n","        layer_name = layer.__class__.__name__\n","        if 'Detect' in layer_name or 'Segment' in layer_name or 'Head' in layer_name:\n","            backbone_end = i\n","            break\n","\n","    backbone_layers = [full_model.model[i] for i in range(backbone_end)]\n","    backbone = nn.Sequential(*backbone_layers)\n","\n","    # 3. Auto-detectar dimens√µes\n","    backbone.eval()\n","    with torch.no_grad():\n","        dummy_input = torch.randn(1, 3, 224, 224)\n","        if torch.cuda.is_available():\n","            dummy_input = dummy_input.cuda()\n","            backbone = backbone.cuda()\n","\n","        features = backbone(dummy_input)\n","        if isinstance(features, (list, tuple)):\n","            features = features[-1]\n","\n","        in_features = features.shape[1]\n","\n","    backbone.train()\n","\n","    # 4. Criar cabe√ßa de classifica√ß√£o (MESMA do treinamento!)\n","    classifier_head = nn.Sequential(\n","        nn.AdaptiveAvgPool2d((1, 1)),\n","        nn.Flatten(),\n","        nn.BatchNorm1d(in_features),\n","        nn.Dropout(0.3),\n","        nn.Linear(in_features, 256),\n","        nn.ReLU(inplace=True),\n","        nn.BatchNorm1d(256),\n","        nn.Dropout(0.2),\n","        nn.Linear(256, num_classes)\n","    )\n","\n","    # 5. Modelo completo\n","    full_classifier = nn.Sequential(backbone, classifier_head)\n","\n","    return full_classifier\n","\n","# ================= Carregar Modelos =================\n","def load_frozen_model(weight_path):\n","    model = create_model()\n","    model.load_state_dict(torch.load(weight_path, map_location=DEVICE))\n","    model.to(DEVICE)\n","    model.eval()\n","    for p in model.parameters():\n","        p.requires_grad = False\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rOrheL3HlsEf"},"outputs":[],"source":["# ================= Avalia√ß√£o =================\n","@torch.no_grad()\n","def evaluate_ensemble(model_folha, model_casca, loader):\n","    all_preds = []\n","    all_labels = []\n","    inference_times = []  # ‚Üê NOVO\n","\n","    softmax = nn.Softmax(dim=1)\n","\n","    for img_f, img_c, labels in tqdm(loader, desc=\"Infer√™ncia Ensemble\"):\n","        img_f = img_f.to(DEVICE)\n","        img_c = img_c.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        # ‚Üê NOVO: Medir tempo\n","        start_time = time.time()\n","\n","        out_f = softmax(model_folha(img_f))\n","        out_c = softmax(model_casca(img_c))\n","\n","        probs = out_f + out_c\n","        preds = torch.argmax(probs, dim=1)\n","\n","        end_time = time.time()\n","        batch_time = end_time - start_time\n","        inference_times.append(batch_time / img_f.size(0))  # Tempo por imagem\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    acc = accuracy_score(all_labels, all_preds)\n","    f1  = f1_score(all_labels, all_preds, average=\"weighted\")\n","    avg_time = np.mean(inference_times) * 1000  # ‚Üê NOVO: em milissegundos\n","    return acc, f1, avg_time,  all_preds, all_labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nNuqz0-Opvl3"},"outputs":[],"source":["# ================= Avalia√ß√£o Individual =================\n","@torch.no_grad()\n","def evaluate_single_older_version(model, loader, modality=\"folha\"):\n","    \"\"\"\n","    Avalia modelo individual (s√≥ folhas OU s√≥ cascas).\n","\n","    Args:\n","        modality: \"folha\" ou \"casca\"\n","    \"\"\"\n","    all_preds = []\n","    all_labels = []\n","    inference_times = []  # ‚Üê NOVO\n","\n","    for img_f, img_c, labels in tqdm(loader, desc=f\"Eval {modality}\"):\n","        # Escolher modalidade\n","        img = img_f if modality == \"folha\" else img_c\n","        img = img.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        # ‚Üê NOVO: Medir tempo\n","        start_time = time.time()\n","\n","        output = model(img)\n","        preds = torch.argmax(output, dim=1)\n","\n","        end_time = time.time()\n","        batch_time = end_time - start_time\n","        inference_times.append(batch_time / img.size(0))\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    acc = accuracy_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n","    avg_time = np.mean(inference_times) * 1000  # ‚Üê NOVO: em milissegundos\n","    return acc, f1, avg_time, all_preds, all_labels"]},{"cell_type":"code","source":["# ================= Avalia√ß√£o Individual CORRETA =================\n","@torch.no_grad()\n","def evaluate_single(model, loader):\n","    \"\"\"\n","    Avalia modelo em dataset UNIMODAL (sem pares).\n","    \"\"\"\n","    all_preds = []\n","    all_labels = []\n","    inference_times = []\n","\n","    for batch in tqdm(loader, desc=\"Infer√™ncia\"):\n","        # Loader unimodal retorna (img, label) n√£o (img_f, img_c, label)\n","        if len(batch) == 2:\n","            img, labels = batch\n","        else:\n","            # Se for bimodal, desempacotar\n","            img, labels = batch[0], batch[2]\n","\n","        img = img.to(DEVICE)\n","        labels = labels.to(DEVICE)\n","\n","        start_time = time.time()\n","        output = model(img)\n","        preds = torch.argmax(output, dim=1)\n","        end_time = time.time()\n","\n","        batch_time = end_time - start_time\n","        inference_times.append(batch_time / img.size(0))\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    acc = accuracy_score(all_labels, all_preds)\n","    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n","    avg_time = np.mean(inference_times) * 1000\n","\n","    return acc, f1, avg_time, all_preds, all_labels"],"metadata":{"id":"uXo1ZbDCqOkm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ================= Fun√ß√µes de An√°lise =================\n","def plot_confusion_matrix(y_true, y_pred, class_names, title=\"Matriz de Confus√£o\", normalize=False):\n","    \"\"\"\n","    Plota matriz de confus√£o com visualiza√ß√£o aprimorada.\n","\n","    Args:\n","        y_true: Labels verdadeiros\n","        y_pred: Predi√ß√µes do modelo\n","        class_names: Lista com nomes das classes\n","        title: T√≠tulo do gr√°fico\n","        normalize: Se True, normaliza os valores por linha (%) - (Importante! As outras matrizes dos modelos simples est√£o como n√∫mero bruto)\n","    \"\"\"\n","    cm = confusion_matrix(y_true, y_pred)\n","    #df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        fmt = '.2%'\n","    else:\n","        fmt = 'd'\n","\n","    plt.figure(figsize=(12, 10))\n","    sns.heatmap(cm, annot=True, fmt=fmt, cmap='Blues',\n","                xticklabels=class_names, yticklabels=class_names,\n","                cbar_kws={'label': 'Porcentagem' if normalize else 'Contagem'})\n","    plt.title(title, fontsize=14, fontweight='bold')\n","    plt.ylabel('Classe Verdadeira', fontsize=12)\n","    plt.xlabel('Classe Predita', fontsize=12)\n","    plt.xticks(rotation=45, ha='right')\n","    plt.yticks(rotation=0)\n","    plt.tight_layout()\n","    plt.show()\n","\n","def print_per_class_accuracy(y_true, y_pred, class_names):\n","    \"\"\"\n","    Imprime acur√°cia por classe e outras m√©tricas detalhadas.\n","\n","    Args:\n","        y_true: Labels verdadeiros\n","        y_pred: Predi√ß√µes do modelo\n","        class_names: Lista com nomes das classes\n","    \"\"\"\n","    cm = confusion_matrix(y_true, y_pred)\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"ACUR√ÅCIA POR CLASSE\")\n","    print(\"=\"*80)\n","    print(f\"{'Classe':<25} {'Corretas':>10} {'Total':>10} {'Acur√°cia':>12}\")\n","    print(\"-\"*80)\n","\n","    per_class_acc = []\n","    for i, class_name in enumerate(class_names):\n","        correct = cm[i, i]\n","        total = cm[i, :].sum()\n","        acc = correct / total if total > 0 else 0\n","        per_class_acc.append(acc)\n","        print(f\"{class_name:<25} {correct:>10} {total:>10} {acc:>12.2%}\")\n","\n","    print(\"-\"*80)\n","    print(f\"{'M√âDIA':<25} {'':<10} {'':<10} {np.mean(per_class_acc):>12.2%}\")\n","    print(\"=\"*80)\n","\n","    # Relat√≥rio de classifica√ß√£o detalhado\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"RELAT√ìRIO DE CLASSIFICA√á√ÉO DETALHADO\")\n","    print(\"=\"*80)\n","    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))\n","\n","    return per_class_acc"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aKOViqQWyaON","executionInfo":{"status":"ok","timestamp":1770312747563,"user_tz":180,"elapsed":23,"user":{"displayName":"Marcos Vin√≠cius","userId":"03427921222894808901"}},"outputId":"c58a6e4e-b77b-4517-a2f3-ed6179f94f3f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('all_preds,', 'all_labels')"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BG3qYvcjkRh_"},"outputs":[],"source":["# ================= Fun√ß√£o de Predi√ß√£o para Teste =================\n","\n","def predict_single_pair(model_folha, model_casca, img_folha_path, img_casca_path, class_names=None):\n","    \"\"\"\n","    Faz predi√ß√£o em um √∫nico par de imagens (folha + casca).\n","\n","    Args:\n","        model_folha: Modelo treinado para folhas\n","        model_casca: Modelo treinado para cascas\n","        img_folha_path: Caminho da imagem de folha\n","        img_casca_path: Caminho da imagem de casca\n","        class_names: Lista com nomes das classes (opcional)\n","\n","    Returns:\n","        dict com predi√ß√µes e probabilidades de cada modelo\n","    \"\"\"\n","    import time\n","\n","    # Preprocessar imagens\n","    def preprocess(img_path):\n","        img = cv2.imread(img_path)\n","        if img is None:\n","            raise ValueError(f\"N√£o foi poss√≠vel ler: {img_path}\")\n","\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        img = cv2.resize(img, (INPUT_SIZE, INPUT_SIZE))\n","        img = img.astype(np.float32) / 255.0\n","\n","        mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n","        std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n","        img = (img - mean) / std\n","\n","        img = np.transpose(img, (2, 0, 1))\n","        return torch.from_numpy(img).unsqueeze(0)  # Adiciona dimens√£o de batch\n","\n","    # Preprocessar\n","    img_f = preprocess(img_folha_path).to(DEVICE)\n","    img_c = preprocess(img_casca_path).to(DEVICE)\n","\n","    softmax = nn.Softmax(dim=1)\n","\n","    # Infer√™ncia\n","    with torch.no_grad():\n","        start_time = time.time()\n","\n","        # Modelo de folhas\n","        out_f = model_folha(img_f)\n","        prob_f = softmax(out_f).cpu().numpy()[0]\n","        pred_f = np.argmax(prob_f)\n","\n","        # Modelo de cascas\n","        out_c = model_casca(img_c)\n","        prob_c = softmax(out_c).cpu().numpy()[0]\n","        pred_c = np.argmax(prob_c)\n","\n","        # Ensemble (soma)\n","        prob_ensemble = (prob_f + prob_c) / 2.0\n","        pred_ensemble = np.argmax(prob_ensemble)\n","\n","        end_time = time.time()\n","        inference_time = (end_time - start_time) * 1000  # ms\n","\n","    # Preparar resultado\n","    result = {\n","        'folha': {\n","            'classe_pred': int(pred_f),\n","            'nome_classe': class_names[pred_f] if class_names else f\"Classe_{pred_f}\",\n","            'confianca': float(prob_f[pred_f]),\n","            'probabilidades': prob_f.tolist()\n","        },\n","        'casca': {\n","            'classe_pred': int(pred_c),\n","            'nome_classe': class_names[pred_c] if class_names else f\"Classe_{pred_c}\",\n","            'confianca': float(prob_c[pred_c]),\n","            'probabilidades': prob_c.tolist()\n","        },\n","        'ensemble': {\n","            'classe_pred': int(pred_ensemble),\n","            'nome_classe': class_names[pred_ensemble] if class_names else f\"Classe_{pred_ensemble}\",\n","            'confianca': float(prob_ensemble[pred_ensemble]),\n","            'probabilidades': prob_ensemble.tolist()\n","        },\n","        'tempo_inferencia_ms': float(inference_time),\n","        'concordancia': (pred_f == pred_c == pred_ensemble)\n","    }\n","\n","    # Exibir resultado formatado\n","    print(f\"\\n{'='*70}\")\n","    print(f\"PREDI√á√ÉO - PAR DE IMAGENS\")\n","    print(f\"{'='*70}\")\n","    print(f\"Folha: {img_folha_path}\")\n","    print(f\"Casca: {img_casca_path}\")\n","    print(f\"{'-'*70}\")\n","    print(f\"{'Modelo':<15} {'Predi√ß√£o':<25} {'Confian√ßa':>12}\")\n","    print(f\"{'-'*70}\")\n","    print(f\"{'Folhas':<15} {result['folha']['nome_classe']:<25} {result['folha']['confianca']:>12.2%}\")\n","    print(f\"{'Cascas':<15} {result['casca']['nome_classe']:<25} {result['casca']['confianca']:>12.2%}\")\n","    print(f\"{'-'*70}\")\n","    print(f\"{'ENSEMBLE':<15} {result['ensemble']['nome_classe']:<25} {result['ensemble']['confianca']:>12.2%}\")\n","    print(f\"{'='*70}\")\n","    print(f\"Tempo de infer√™ncia: {result['tempo_inferencia_ms']:.2f} ms\")\n","\n","    if result['concordancia']:\n","        print(f\"‚úÖ Todos os modelos CONCORDAM na predi√ß√£o!\")\n","    else:\n","        print(f\"‚ö†Ô∏è  Modelos DISCORDAM:\")\n","        if pred_f != pred_ensemble:\n","            print(f\"   - Folhas prev√™: {result['folha']['nome_classe']}\")\n","        if pred_c != pred_ensemble:\n","            print(f\"   - Cascas prev√™: {result['casca']['nome_classe']}\")\n","        print(f\"   - Ensemble decidiu: {result['ensemble']['nome_classe']}\")\n","\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v0zrpn9VnK1m"},"outputs":[],"source":["# ================= Main 1 =================\n","\n","if __name__ == \"__main__\":\n","    print(\"\\n--- Modelo H√≠brido 1 (YOLOv12): Decision-Level Fusion ---\\n\")\n","\n","    # 1. Dividir datasets originais\n","    ds_f = ImageFolderDataset(DATASET_FOLHA)\n","    print(f\"Dataset de Folhas:\\nClasses detectadas ({len(ds_f.class_to_idx.keys())}): {ds_f.class_to_idx.keys()}\")\n","    print(f\"Total de Imagens: {len(ds_f)}\")\n","    train_f, valid_f, test_f = stratified_split(ds_f)\n","    print(f\"Total: {len(train_f)+len(valid_f)+len(test_f)} | Treino: {len(train_f)} | Valida√ß√£o: {len(valid_f)} | Teste: {len(test_f)}\\n\")\n","\n","    ds_c = ImageFolderDataset(DATASET_CASCA)\n","    print(f\"Dataset de Cascas:\\nClasses detectadas ({len(ds_c.class_to_idx.keys())}): {ds_c.class_to_idx.keys()}\")\n","    print(f\"Total de Imagens: {len(ds_c)}\")\n","    train_c, valid_c, test_c = stratified_split(ds_c)\n","    print(f\"Total: {len(train_c)+len(valid_c)+len(test_c)} | Treino: {len(train_c)} | Valida√ß√£o: {len(valid_c)} | Teste: {len(test_c)}\\n\")\n","\n","    # 2. Criar produto cartesiano DEPOIS\n","    train_fusion = CartesianFusionDataset(train_f, train_c)\n","    valid_fusion = CartesianFusionDataset(valid_f, valid_c)\n","    test_fusion = CartesianFusionDataset(test_f, test_c)\n","\n","    #fusion = CartesianFusionDataset(ds_f, ds_c)\n","    #train_fusion, valid_fusion, test_fusion = stratified_split(fusion)\n","\n","    # Extra. Exibir dados do Dataset ---------------------------------------- DEBUG ------------------------------------------------------------------------------------------\n","    print(f\"Total: {len(train_fusion)+len(valid_fusion)+len(test_fusion)} | Treino: {len(train_fusion)} | Valida√ß√£o: {len(valid_fusion)} | Teste: {len(test_fusion)}\")\n","\n","    print(\"\\n=== DEBUG ===\")\n","\n","    # Verificar quantas classes est√£o presentes em cada split\n","    def check_classes(fusion_ds, name):\n","        classes_presentes = set()\n","        for _, _, label in fusion_ds.samples:\n","            classes_presentes.add(label)\n","        print(f\"{name}: {len(classes_presentes)} classes presentes de 15 totais\")\n","        print(f\"Classes: {sorted(classes_presentes)}\")\n","        return classes_presentes\n","\n","    train_classes = check_classes(train_fusion, \"Train\")\n","    valid_classes = check_classes(valid_fusion, \"Valid\")\n","    test_classes = check_classes(test_fusion, \"Test\")\n","\n","    # Verificar se h√° classes faltando\n","    all_classes = set(range(15))\n","    print(f\"\\nClasses faltando em train: {all_classes - train_classes}\")\n","    print(f\"Classes faltando em valid: {all_classes - valid_classes}\")\n","    print(f\"Classes faltando em test: {all_classes - test_classes}\")\n","\n","    # Pares por classe em cada split\n","    def count_pairs_per_class(fusion_ds, name):\n","        pares_por_classe = {}\n","        for _, _, label in fusion_ds.samples:\n","            pares_por_classe[label] = pares_por_classe.get(label, 0) + 1\n","\n","        print(f\"\\n{name} - Pares por classe:\")\n","        for label in sorted(pares_por_classe.keys()):\n","            print(f\"  Classe {label}: {pares_por_classe[label]:4d} pares\")\n","        print(f\"Total: {sum(pares_por_classe.values())}\")\n","\n","    count_pairs_per_class(train_fusion, \"TREINO\")\n","    count_pairs_per_class(valid_fusion, \"VALIDA√á√ÉO\")\n","    count_pairs_per_class(test_fusion, \"TESTE\")\n","\n","    # -------------------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","    # 3. Criar DataLoaders\n","    train_loader = DataLoader(train_fusion, batch_size=BATCH_SIZE, shuffle=True)\n","    valid_loader = DataLoader(valid_fusion, batch_size=BATCH_SIZE, shuffle=False)\n","    test_loader = DataLoader(test_fusion, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    test_loader_single_f = DataLoader(test_f, batch_size=BATCH_SIZE, shuffle=False)\n","    test_loader_single_c = DataLoader(test_c, batch_size=BATCH_SIZE, shuffle=False)\n","\n","    model_f = load_frozen_model(PESOS_FOLHA)\n","    model_c = load_frozen_model(PESOS_CASCA)\n","\n","    # Obter nomes das classes\n","    class_names = list(ds_f.class_to_idx.keys())\n","\n","    # Avaliar modelos individuais\n","    print(\"Avaliando modelo de FOLHAS...\")\n","    #acc_f, f1_f, time_f = evaluate_single(model_f, test_loader, modality=\"folha\")\n","    acc_f, f1_f, time_f, preds_f, labels_f = evaluate_single(model_f, test_loader_single_f)\n","\n","    print(\"Avaliando modelo de CASCAS...\")\n","    #acc_c, f1_c, time_c = evaluate_single(model_c, test_loader, modality=\"casca\")\n","    acc_c, f1_c, time_c, preds_c, labels_c = evaluate_single(model_c, test_loader_single_c)\n","\n","    # Avaliar ensemble\n","    print(\"Avaliando ENSEMBLE...\")\n","    acc_e, f1_e, time_e, preds_e, labels_e = evaluate_ensemble(model_f, model_c, test_loader)\n","\n","    # Compara√ß√£o\n","    print(f\"\\n{'='*80}\")\n","    print(f\"RESULTADOS FINAIS - TESTE\")\n","    print(f\"{'='*80}\")\n","    print(f\"{'Modelo':<20} {'Accuracy':>12} {'F1-Score':>12} {'Tempo (ms)':>15}\")\n","    print(f\"{'-'*80}\")\n","    print(f\"{'Folhas':<20} {acc_f:>12.4f} {f1_f:>12.4f} {time_f:>15.2f}\")\n","    print(f\"{'Cascas':<20} {acc_c:>12.4f} {f1_c:>12.4f} {time_c:>15.2f}\")\n","    print(f\"{'-'*80}\")\n","    print(f\"{'ENSEMBLE':<20} {acc_e:>12.4f} {f1_e:>12.4f} {time_e:>15.2f}\")\n","    print(f\"{'='*80}\")\n","\n","    # Calcular ganho\n","    best_individual = max(acc_f, acc_c)\n","    gain = acc_e - best_individual\n","    gain_pct = (gain / best_individual) * 100\n","\n","    if gain > 0:\n","        print(f\"\\n‚úÖ Ensemble MELHOROU em {gain:.4f} (+{gain_pct:.2f}%)\")\n","    else:\n","        print(f\"\\n‚ö†Ô∏è  Ensemble N√ÉO melhorou ({gain:.4f} / {gain_pct:.2f}%)\")\n","\n","    # ‚Üê NOVO: Compara√ß√£o de tempo\n","    print(f\"\\n‚è±Ô∏è  TEMPO DE INFER√äNCIA:\")\n","    print(f\"   Folhas:   {time_f:.2f} ms/imagem\")\n","    print(f\"   Cascas:   {time_c:.2f} ms/imagem\")\n","    print(f\"   Ensemble: {time_e:.2f} ms/imagem ({time_e/max(time_f, time_c):.2f}x mais lento)\")\n","\n","    # ================= AN√ÅLISES DETALHADAS =================\n","\n","    # 1. An√°lise do modelo de FOLHAS\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"AN√ÅLISE DETALHADA - MODELO DE FOLHAS\")\n","    print(\"=\"*80)\n","    print_per_class_accuracy(labels_f, preds_f, class_names)\n","    plot_confusion_matrix(labels_f, preds_f, class_names,\n","                         title=\"Matriz de Confus√£o - Modelo de Folhas\",\n","                         normalize=True)\n","\n","    # 2. An√°lise do modelo de CASCAS\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"AN√ÅLISE DETALHADA - MODELO DE CASCAS\")\n","    print(\"=\"*80)\n","    print_per_class_accuracy(labels_c, preds_c, class_names)\n","    plot_confusion_matrix(labels_c, preds_c, class_names,\n","                         title=\"Matriz de Confus√£o - Modelo de Cascas\",\n","                         normalize=True)\n","\n","    # 3. An√°lise do ENSEMBLE\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"AN√ÅLISE DETALHADA - ENSEMBLE\")\n","    print(\"=\"*80)\n","    print_per_class_accuracy(labels_e, preds_e, class_names)\n","    plot_confusion_matrix(labels_e, preds_e, class_names,\n","                         title=\"Matriz de Confus√£o - Ensemble (Decision-Level Fusion)\",\n","                         normalize=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3qGCBwWwkalC"},"outputs":[],"source":["# ================= Main 2 =================\n","\n","if __name__ == \"__main__\":\n","    print(\"\\n--- Modelo H√≠brido 1 (YOLOv12): Decision-Level Fusion ---\\n\")\n","\n","    # 1. Dividir datasets originais\n","    ds_f = ImageFolderDataset(DATASET_FOLHA)\n","    ds_c = ImageFolderDataset(DATASET_CASCA)\n","\n","     # Obter nomes das classes\n","    ds_f = ImageFolderDataset(DATASET_FOLHA)\n","    class_names = list(ds_f.class_to_idx.keys())\n","\n","    # TESTE: Predizer um par espec√≠fico\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"TESTE DE PREDI√á√ÉO INDIVIDUAL\")\n","    print(\"=\"*70)\n","\n","    # Exemplo: pegar uma imagem de teste\n","    #folha_teste = \"/content/drive/MyDrive/TCC/Datasets/Imagens Folhas/Especies/Coite/Coite R1.jpg\"\n","    folha_teste = \"/content/drive/MyDrive/TCC/Datasets/Testes/Jambolao G25.jpg\"\n","    #casca_teste = \"/content/drive/MyDrive/TCC/Datasets/Imagens tronco/EspeciesCascas/Coite/Coite B3.jpg\"\n","    casca_teste = \"/content/drive/MyDrive/TCC/Datasets/Testes/Caterete D14.jpg\"\n","\n","    model_f = load_frozen_model(PESOS_FOLHA)\n","    model_c = load_frozen_model(PESOS_CASCA)\n","\n","    resultado = predict_single_pair(\n","        model_f,\n","        model_c,\n","        folha_teste,\n","        casca_teste,\n","        class_names=class_names\n","    )\n","\n","    # Acessar resultados programaticamente\n","    print(f\"\\nüìä Detalhes da Predi√ß√£o:\")\n","    print(f\"   Classe final: {resultado['ensemble']['nome_classe']}\")\n","    print(f\"   Confian√ßa: {resultado['ensemble']['confianca']:.2%}\")\n","    print(f\"   Top 3 classes:\")\n","\n","    # Mostrar top 3\n","    probs_ensemble = resultado['ensemble']['probabilidades']\n","    probs_casca = resultado['casca']['probabilidades']\n","    probs_folha = resultado['folha']['probabilidades']\n","    #top3_indices = np.argsort(probs_ensemble)[::-1][:3]\n","    top3_indices = np.argsort(probs_ensemble)[::-1][:15]\n","    for i, idx in enumerate(top3_indices, 1):\n","        print(f\"      {i}. {class_names[idx]}: {probs_ensemble[idx]:.2%} (Casca: {probs_casca[idx]:.2%} + Folha: {probs_folha[idx]:.2%})\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}